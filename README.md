# Link-Context Learning for Multimodal LLMs

<p align="center" width="100%">
<img src="ISEKAI_overview.png"  width="80%" height="80%">
</p>

<div>
<div align="center">
    <a href='https://macavityt.github.io/' target='_blank'>Yan Tai<sup>*,1,2</sup></a>&emsp;
    <a href='https://weichenfan.github.io/Weichen/' target='_blank'>Weichen Fan<sup>*,â€ ,1</sup></a>&emsp;
    <a href='https://zhaozhang.net/' target='_blank'>Zhao Zhang<sup>1</sup></a>&emsp;
    <a href='https://zhufengx.github.io/' target='_blank'>Feng Zhu<sup>1</sup></a>&emsp;
    <a href='https://scholar.google.com/citations?user=1c9oQNMAAAAJ&hl=zh-CN' target='_blank'>Rui Zhao<sup>1</sup></a>&emsp;
    <a href='https://liuziwei7.github.io/' target='_blank'>Ziwei Liu<sup>&#x2709,3</sup></a>
</div>
<div>
<div align="center">
    <sup>1</sup>SenseTime Research&emsp;
    <sup>2</sup>Institute of Automation, CAS&emsp;
    <sup>3</sup>S-Lab, Nanyang Technological University&emsp;
    </br>
    <sup>*</sup> Equal Contribution&emsp;
    <sup>â€ </sup> Project Lead&emsp;
    <sup>&#x2709</sup> Corresponding Author
    
</div>
 
 -----------------

![](https://img.shields.io/badge/ISEKAI-v0.1-darkcyan)
![](https://img.shields.io/github/stars/isekai-portal/Link-Context-Learning)
![](https://black.readthedocs.io/en/stable/_static/license.svg)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fisekai-portal%2FLink-Context-Learning&count_bg=%23BDC4B7&title_bg=%2342C4A8&icon=octopusdeploy.svg&icon_color=%23E7E7E7&title=visitors&edge_flat=true)](https://hits.seeyoufarm.com)
[![Dataset](https://img.shields.io/badge/Dataset-Download-blue)](https://huggingface.co/ISEKAI-Portal) 
[![Generic badge](https://img.shields.io/badge/DEMO-LCL_Demo-<COLOR>.svg)](http://117.144.81.99:20488/)


## Updates
- **24 Aug, 2023**: :boom::boom: We release the online demo at [ğŸ”—LCL-DemoğŸ”—](http://117.144.81.99:20488/).
- **17 Aug, 2023**: :boom::boom: We release the two subsets of ISEKAI (ISEKAI-10 and ISEKAI-pair) at [[Hugging Face ğŸ¤—]](https://huggingface.co/ISEKAI-Portal).

---
This repository contains the **official implementation** and **dataset** of the following paper:

> **Link-Context Learning for Multimodal LLMs**<br>
> https://arxiv.org/abs/2308.07891
>
> **Abstract:** *The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to "learn to learn" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes "reasoning from cause and effect" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs.*

  
## Todo

1. [x] Release the [ISEKAI-10](https://huggingface.co/datasets/ISEKAI-Portal/ISEKAI-10) and [ISEKAI-pair](https://huggingface.co/datasets/ISEKAI-Portal/ISEKAI-pair).
2. [ ] Release the dataset usage.
3. [x] Release the demo.
4. [ ] Release the codes and checkpoints.
5. [ ] Release the full ISEKAI dataset.


## Cite

```bibtex
@article{tai2023link,
  title={Link-Context Learning for Multimodal LLMs},
  author={Tai, Yan and Fan, Weichen and Zhang, Zhao and Zhu, Feng and Zhao, Rui and Liu, Ziwei},
  journal={arXiv preprint arXiv:2308.07891},
  year={2023}
}
```

## GET START

[TOC]
ç»Ÿä¸€éƒ¨åˆ†å¼€æºå¤šæ¨¡æ€LLMæ•°æ®æ¥å£ã€è®­ç»ƒ/æ¨ç†è„šæœ¬ï¼Œæ–¹ä¾¿å¿«é€Ÿåˆ‡æ¢æ•°æ®é›†/æ¨¡å‹è¿›è¡ŒéªŒè¯ã€‚



## Datasets

- [ ] ScienceQA
- [x] ref4-gnome
- [x] ref3-coco
- [x] ref_reverse(grounded caption)
- [x] flickr30k
- [x] flickr30k_reverse(multi object grounded caption)
- [x] point3(point-local, point-twice, point-v7w)
- [x] gqa
- [x] clevr

## Models

- [x] llava_v1_7b
- [x] openflamingo(only support training in otter format. i.e. add\<answer>token)
- [x] otter

## Getting Started

### Dependence Installation

#### 1. å®‰è£…torch/torchvision/transformers

A100 å‚è€ƒ [LLaVAè®­ç»ƒç¯å¢ƒä¾èµ–](https://www.yuque.com/z_zhang/ab73nw/rwxn03tibq0kw15e). 

V100 å‚è€ƒ [LLaVAè®­ç»ƒç¯å¢ƒä¾èµ– 5. Pytorch 6. Transformers](https://www.yuque.com/z_zhang/ab73nw/rwxn03tibq0kw15e). V100ä¸æ”¯æŒflash-attn, å¯ä»¥ä¸ç”¨é…ç½®.

#### 2. å®‰è£…å…¶ä»–ä¾èµ–

```shell
pip install -r requirements.txt
```

#### 3. é…ç½®accelerate

##### option 1 ä½¿ç”¨å‘½ä»¤è¡Œé…ç½®

```shell
accelerate config
```

æ ¹æ®æç¤ºé…ç½®

##### option 2 å¤åˆ¶é»˜è®¤é…ç½®

å°†é»˜è®¤é…ç½®å¤åˆ¶åˆ°ç›®æ ‡æ–‡ä»¶å¤¹ä¸‹

```shell
mkdir -p ~/.cache/huggingface/accelerate
cp accelerate_config/default_config.yaml ~/.cache/huggingface/accelerate/default_config.yaml
```

#### 4. é…ç½®ceph(å¯é€‰)

éœ€è¦ç”¨åˆ°cephæ–‡ä»¶ç³»ç»Ÿæ—¶é…ç½®

```shell
python -m pip install setuptools==59.2.0
python -m pip install pip==21.3.1
python -m pip install -i http://pkg.sensetime.com/repository/pypi-proxy/simple/ --trusted-host pkg.sensetime.com http://10.5.41.14/packages/petrel-oss-sdk.tar.gz
```

æµ‹è¯•cephæ˜¯å¦å®‰è£…æˆåŠŸï¼š

```shell
python -c 'from petrel_client.version import version; print(version)'
```

æŸ¥çœ‹petrel_clientå®‰è£…è·¯å¾„ï¼Œç¡®ä¿åœ¨cacheæ–‡ä»¶ç³»ç»Ÿä¸Š

```shell
python -c 'import petrel_client; print(petrel_client.__file__)'
```

### æ•°æ®å¤„ç†

éœ€è¦å®ç°Datasetç±»ï¼Œåœ¨è°ƒç”¨`__getitem__`æ—¶è¿”å›å¦‚ä¸‹æ ¼å¼çš„item. ä¹‹åçš„æ ¼å¼è½¬æ¢SingleImageConvDatasetä¼šä¸ºä½ å®Œæˆï¼Œç»†èŠ‚è§ mllm/dataset/single_image_convsation.py: class SingleImageConvDataset.

```python
item = {
    'image': None,  # PIL.Image.Image,
    'target': {
        # xmin, ymin, xmax, ymax
        'boxes': [
            [10, 10, 256, 265],  # dog1
            [24, 18, 378, 768],  # dog2
            [100, 310, 670, 653],  # man
            [278, 320, 809, 673],  # rope
        ],
        'points': [
            [100, 100],  # man1
            [200, 200],  # man2
        ]
    },

    "conversations": [
        {
            'from': 'human',
            'value': 'What is the relation between the two dogs <boxes> and the man <boxes> in the image <image> ? Is the man<points> shaking hands with the man<points>?',
            'boxes_seq': [[0, 1], [2], ],
            'points_seq': [[0,], [1,]],
        },
        {
            'from': 'gpt',
            'value': 'a rope <boxes> is connecting the left dog <boxes> with the man <boxes>. '
                     'So the man <boxes> is walking the dog <boxes>.'
                    'And the man <boxes> has no relationship with the right dog <boxes>',
            'boxes_seq': [[3], [0], [2], [2], [0], [2], [1]],
            'points_seq': None,
        }
    ]
}
```

### Training

#### åœ¨RECä¸Šè®­ç»ƒLLaVA

```shell
sbatch -p A10080G-share \
    -n 1 \
    -N 1 \
    --gres=gpu:4 \
    -c 64 \
    accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/rec_llava_v1_7b_fsdp.py
```

å¦‚æœmain_process_portè¢«å ç”¨ï¼Œéšä¾¿æ¢ä¸€ä¸ªå°±å¥½

#### ä½¿ç”¨flash-attenè¿›è¡Œè®­ç»ƒï¼ˆä»…A100æ”¯æŒï¼‰

```shell
sbatch -p A10080G-share \
    -n 1 \
    -N 1 \
    --gres=gpu:4 \
    -c 64 \
    accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune_mem.py \
        config/rec_llava_v1_7b_fsdp.py
```

å³ï¼šå°†è„šæœ¬ä»finetune.pyåˆ‡æ¢ä¸ºfinetune_mem.py. ä½¿ç”¨flash-attenè¿›è¡ŒéªŒè¯åŒç†.

#### åœ¨RECä¸Šè®­ç»ƒæŒ‡å®šckptï¼Œå¹¶æ›´æ”¹epoch learning_rateç­‰å‚æ•°

```shell
sbatch -p A10080G-share \
    -n 1 \
    -N 1 \
    --gres=gpu:4 \
    -c 64 \
    accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/rec_llava_v1_7b_fsdp.py \
            --cfg-options model_args.model_name_or_path='/path/to/new/ckpt' \
            --num_train_epochs 1 \
            --learning_rate 4e-5
```

æ”¯æŒTransformers.Seq2SeqArgumentsçš„æ‰€æœ‰å‚æ•°, å¯ä»¥ç›´æ¥--xxx  valueè¿›è¡Œæ›´æ”¹, è¿™éƒ¨åˆ†å‚æ•°å’Œæ§åˆ¶è®­ç»ƒ/æ¨ç†è¡Œä¸ºç›¸å…³. å¯¹äºå…¶ä¸­ä¸ç»å¸¸å˜åŠ¨çš„å‚æ•°, ä¹Ÿå¯ä»¥å†™åˆ°training_argsä¸­å­˜åˆ°æ–‡ä»¶ä¸­, è„šæœ¬ä¼šè‡ªåŠ¨å°†è¿™äº›å‚æ•°åˆå¹¶èµ·æ¥é€ç»™Transformers.Seq2SeqArguments. å®ç°è¯¦è§mllm/config/config.py

æ”¯æŒmmengineç±»ä¼¼çš„ä»æ–‡ä»¶ä¸­è¯»å–å‚æ•°ï¼Œè¦ä¿®æ”¹è¯¥éƒ¨åˆ†å‚æ•°ï¼Œè¦ä½¿ç”¨--cfg-options xxx1=value xxx2=valueè¿›è¡Œæ›´æ”¹ï¼Œè¿™éƒ¨åˆ†å‚æ•°å’Œæ¨¡å‹ã€æ•°æ®ç­‰å…¶ä»–è¡Œä¸ºç›¸å…³

### Evaluation & Predict

#### åœ¨RECä¸Šæ¨ç†LLaVA

```shell
sbatch -p A10080G-share \
    -n 1 \
    -N 1 \
    --gres=gpu:4 \
    -c 64 \
    accelerate launch --num_processes 4 \
        --main_process_port 23786 \
        mllm/pipeline/finetune.py \
        config/rec_llava_v1_7b_fsdp_eval.py
```

æœ¬è´¨ä¸Šæ˜¯ç”±Transformers.Seq2SeqArgumentsçš„do_train, do_eval, do_predictå‚æ•°åˆ†åˆ«æ§åˆ¶æ˜¯å¦åšè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•. è¯¦æƒ…å‚è€ƒmllm/pipeline/finetune.pyæ–‡ä»¶.

### å¤šæœºå¤šå¡è¿è¡Œ

æ ¸å¿ƒåœ¨äºä½¿ç”¨mpirunåœ¨å¤šä¸ªæœºå™¨ä¸Šåˆ†åˆ«å¯åŠ¨accelerate launchã€‚

#### ä½¿ç”¨4ä¸ªæœºå™¨ï¼ˆ4*8=32å¼ v100è¿›è¡Œé¢„è®­ç»ƒï¼‰

```bash
sbatch --nodes 4 launcher_intelmpi.sh mllm/pipeline/finetune.py config/llava_pretrain5.py
```

#### å¦‚æœæç¤ºç«¯å£è¢«å ç”¨ï¼Œéšä¾¿æ¢ä¸ªMASTER_PORTçš„å€¼å°±å¥½ï¼Œå³ï¼š

```bash
MASTER_PORT=25671 sbatch --nodes 4 launcher_intelmpi.sh mllm/pipeline/finetune.py config/llava_pretrain5.py
```

#### --cfg-options(mmengineé£æ ¼å‚æ•°)å’Œ --do_eval(huggingface:Trainerç±»å‚æ•°)ç­‰é€‰é¡¹å’Œå•æœºæ¨¡å¼ç›¸åŒï¼Œéƒ½æ˜¯ç›´æ¥å¯ä»¥ç”¨çš„

```bash
sbatch --nodes 4 launcher_intelmpi.sh \
        mllm/pipeline/finetune.py \
        config/rec_llava_v1_7b_fsdp.py \
            --cfg-options model_args.model_name_or_path='/path/to/new/ckpt' \
            --num_train_epochs 1 \
            --learning_rate 4e-5
```

**æ³¨æ„**ï¼šè¦ä¿è¯`launcher_intelmpi.sh`å’Œ`start_in_container.sh`æ ¼å¼ä¸ºunix, ä¸”æœ‰è¿è¡Œæƒé™.

```bash
vim launcher_intelmpi.sh
# åœ¨vimä¸­è¾“å…¥ `:set ff` æŸ¥çœ‹æ–‡ä»¶æ ¼å¼
# è‹¥ä¸ºdos, è¯·è¾“å…¥ `:set ff=unix` æ›´æ”¹ä¸ºunixæ ¼å¼
vim start_in_container.sh
# åœ¨vimä¸­è¾“å…¥ `:set ff` æŸ¥çœ‹æ–‡ä»¶æ ¼å¼
# è‹¥ä¸ºdos, è¯·è¾“å…¥ `:set ff=unix` æ›´æ”¹ä¸ºunixæ ¼å¼
chmod +x launcher_intelmpi.sh
chmod +x start_in_container.sh
```

### åœ¨PureVQAæ•°æ®ä¸Šæ¨ç†

```shell
sbatch -p mm_v100_32g \
    -n 1 \
    -N 1 \
    --gres=gpu:4 \
    -c 64 \
    accelerate launch --num_processes 4 --main_process_port 23786 mllm/pipeline/finetune.py \
        config/llava_eval_multi_vqa_yw.py \
        --cfg-options model_args.model_name_or_path=/mnt/lustre/share_data/chenkeqin/dummy_exp_unify_mllm/llava13b_finetune_gpt4gen_qbc/checkpoint-2000 \
        --per_device_eval_batch_size 4 \
        --output_dir /path/to/exp/logging/dir
```

ä¸»è¦æ–‡ä»¶ï¼š

æ¨ç†é…ç½®æ–‡ä»¶ï¼š`config/llava_eval_multi_vqa_yw.py` å…¶ä¸­`multitest`é¡¹æ”¯æŒå¤šä¸ªæ•°æ®é›†çš„æµ‹è¯•æ¨ç†

æ¯æ¨ç†å®Œä¸€ä¸ªæ•°æ®é›†ï¼Œæ¨¡å‹é¢„æµ‹ä¼šæŒ‰é¡ºåºå­˜åˆ°ï¼š`output_dir/multitest_{datasetname}_extra_prediction.jsonl`

å®é™…è°ƒç”¨çš„datasetï¼š`mllm/dataset/single_image_dataset/pure_vqa.py `ã€‚è‹¥æœ‰æ›´ç²¾ç»†çš„éœ€æ±‚å¯ä»¥hackè¿™é‡Œçš„ä»£ç ã€‚

